# backend/tests/test_llm_client_ollama.py

import os
from pathlib import Path
from backend.utils.llm_client import call_llm_with_yaml_prompt

def test_call_ollama_llama32():
    # × ×›×¨×™×— ××ª ×”×¡×¤×§ ×œ×”×™×•×ª ollama
    os.environ["PROVIDER"] = "ollama"
    os.environ["OLLAMA_MODEL"] = "llama3.2"

    prompt_path = Path("backend/prompts/report_prompt.yaml")
    example_input = {
        "business_profile": {
            "area_sqm": 80,
            "seats": 35,
            "uses_gas": True,
            "delivers": True,
            "has_meat": False
        },
        "matched_rules": [
            {
                "title": "×“×¨×™×©×ª ×‘×˜×™×—×•×ª ×‘×’×–",
                "authority": "×›×‘××•×ª",
                "priority": 1,
                "description": "×¢×œ ×”×¢×¡×§ ×œ×•×•×“× ×ª×§×™× ×•×ª ××¢×¨×›×ª ×’×– ×‘×”×ª×× ×œ×ª×§× ×™×."
            }
        ]
    }

    response = call_llm_with_yaml_prompt(
        yaml_path=prompt_path,
        json_input=example_input,
        provider="ollama",   # â† × ×‘×“×§ ×¨×§ ××•×œ Ollama
        verbose=True
    )

    assert isinstance(response, str)
    assert len(response) > 20
    print("\n\nğŸ“„ Ollama Response:\n", response)
